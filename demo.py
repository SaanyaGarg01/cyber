#!/usr/bin/env python3
"""
Demo script to showcase the Cyber Threat Detection System capabilities
This demonstrates the core functionality without requiring full infrastructure
"""

import sys
import os
import json
from datetime import datetime, timedelta

# Add backend to path for imports
sys.path.append(os.path.join(os.path.dirname(__file__), 'backend'))

def demo_language_detection():
    """Demonstrate language detection capabilities"""
    print("üîç LANGUAGE DETECTION DEMO")
    print("=" * 50)
    
    try:
        from app.nlp.language_detection import LanguageDetector
        detector = LanguageDetector()
        
        test_texts = [
            "This is a simple English text about India's economy.",
            "‡§Ø‡§π ‡§è‡§ï ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§µ‡§æ‡§ï‡•ç‡§Ø ‡§π‡•à ‡§ú‡•ã ‡§≠‡§æ‡§∞‡§§ ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§π‡•à‡•§",
            "India accha country hai but problems bhi hain yaar.",  # Hinglish
            "‡§Ü‡§ú ‡§Æ‡•å‡§∏‡§Æ ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à‡•§",
            "Nice weather today, isn't it?"
        ]
        
        for i, text in enumerate(test_texts, 1):
            result = detector.detect_language(text)
            print(f"{i}. Text: {text}")
            print(f"   Language: {result['primary_language']} ({result['confidence']:.2f})")
            print(f"   Mixed: {result['is_mixed']}")
            print()
            
    except ImportError as e:
        print(f"‚ö†Ô∏è  Language detection requires: {e}")
        print("üì¶ Run: pip install langdetect langid")

def demo_stance_detection():
    """Demonstrate stance detection capabilities"""
    print("\nüéØ STANCE DETECTION DEMO")
    print("=" * 50)
    
    try:
        from app.nlp.stance_detection import StanceDetector
        detector = StanceDetector()
        
        test_texts = [
            "India is a great country with rich cultural heritage",
            "India's economy is failing and unemployment is rising",
            "‡§≠‡§æ‡§∞‡§§ ‡§Æ‡§π‡§æ‡§® ‡§π‡•à ‡§î‡§∞ ‡§π‡§Æ‡•á‡§Ç ‡§ó‡§∞‡•ç‡§µ ‡§π‡•ã‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è",
            "India accha hai but government corrupt hai",
            "The weather is really nice today"
        ]
        
        for i, text in enumerate(test_texts, 1):
            result = detector.detect_stance(text)
            print(f"{i}. Text: {text}")
            print(f"   Stance: {result['primary_stance']} ({result['confidence']:.2f})")
            print(f"   Topics: {result['relevant_topics']}")
            print()
            
    except ImportError as e:
        print(f"‚ö†Ô∏è  Stance detection requires: {e}")
        print("üì¶ Some dependencies may need to be installed")

def demo_burst_detection():
    """Demonstrate burst detection capabilities"""
    print("\nüí• BURST DETECTION DEMO")
    print("=" * 50)
    
    try:
        from app.detection.burst_detection import BurstDetector
        detector = BurstDetector()
        
        # Create sample time series data
        base_time = datetime.now() - timedelta(hours=48)
        sample_posts = []
        
        # Normal activity
        for hour in range(48):
            posts_this_hour = 2 if hour not in range(12, 16) else 15  # Burst in hours 12-15
            for post_num in range(posts_this_hour):
                sample_posts.append({
                    'posted_at': base_time + timedelta(hours=hour, minutes=post_num*4),
                    'platform': 'twitter',
                    'hashtags': ['#crisis'] if hour in range(12, 16) else ['#general'],
                    'author': {'username': f'user_{post_num % 5}'}
                })
        
        result = detector.detect_bursts(sample_posts)
        
        print(f"üìä Analyzed {result['total_posts']} posts over {result['time_span_hours']} hours")
        print(f"üî• Kleinberg bursts detected: {len(result['kleinberg_bursts'])}")
        print(f"üìà Z-score anomalies: {len(result['zscore_anomalies'])}")
        print(f"‚ö†Ô∏è  Coordination suspected: {result['coordination_indicators']['suspected_coordination']}")
        print(f"üìä Coordination score: {result['coordination_indicators']['coordination_score']:.2f}")
        
        if result['kleinberg_bursts']:
            burst = result['kleinberg_bursts'][0]
            print(f"\nüéØ First burst details:")
            print(f"   Duration: {burst['duration_hours']} hours")
            print(f"   Intensity: {burst['intensity']:.2f}")
            print(f"   Posts: {burst['total_posts']}")
        
    except ImportError as e:
        print(f"‚ö†Ô∏è  Burst detection requires: {e}")

def demo_campaign_scoring():
    """Demonstrate campaign scoring capabilities"""
    print("\nüèÜ CAMPAIGN SCORING DEMO")
    print("=" * 50)
    
    try:
        from app.detection.campaign_scoring import CampaignScorer
        scorer = CampaignScorer()
        
        # Sample detection results
        sample_results = {
            'toxicity': {
                'toxic_count': 15,
                'total_analyzed': 100,
                'avg_toxicity_score': 0.8
            },
            'stance': {
                'anti_india_count': 25,
                'total_relevant': 80,
                'avg_anti_stance_score': 0.7
            },
            'coordination': {
                'suspected_coordination': True,
                'coordination_score': 0.75,
                'coordinated_users': 30
            },
            'bot_network': {
                'suspected_bots': 20,
                'total_users': 100,
                'avg_bot_score': 0.6
            }
        }
        
        campaign_score = scorer.calculate_campaign_score(sample_results)
        
        print(f"üéØ Overall Campaign Score: {campaign_score['overall_score']:.1f}/100")
        print(f"‚ö†Ô∏è  Threat Level: {campaign_score['threat_level'].upper()}")
        print(f"\nüìä Component Scores:")
        for component, score in campaign_score['component_scores'].items():
            print(f"   {component.replace('_', ' ').title()}: {score:.1f}")
        
        print(f"\nüö® Risk Factors ({len(campaign_score['risk_factors'])}):")
        for risk in campaign_score['risk_factors'][:3]:  # Show top 3
            print(f"   ‚Ä¢ {risk['factor']}: {risk['description']}")
        
    except ImportError as e:
        print(f"‚ö†Ô∏è  Campaign scoring requires: {e}")

def demo_sample_analysis():
    """Demonstrate end-to-end analysis on sample data"""
    print("\nüî¨ SAMPLE DATA ANALYSIS DEMO")
    print("=" * 50)
    
    sample_posts = [
        {
            'text': "India is failing economically, unemployment everywhere!",
            'platform': 'twitter',
            'hashtags': ['#IndiaFailing', '#Economic Crisis'],
            'posted_at': datetime.now() - timedelta(minutes=30)
        },
        {
            'text': "Proud to be Indian! Jai Hind! üáÆüá≥",
            'platform': 'twitter',
            'hashtags': ['#ProudIndian', '#JaiHind'],
            'posted_at': datetime.now() - timedelta(minutes=15)
        },
        {
            'text': "India accha country hai but corruption bahut zyada hai",
            'platform': 'twitter',
            'hashtags': ['#India', '#Corruption'],
            'posted_at': datetime.now() - timedelta(minutes=5)
        }
    ]
    
    print("üìù Sample Posts Analysis:")
    for i, post in enumerate(sample_posts, 1):
        print(f"\n{i}. {post['text']}")
        print(f"   Platform: {post['platform']}")
        print(f"   Hashtags: {', '.join(post['hashtags'])}")
        print(f"   Posted: {post['posted_at'].strftime('%Y-%m-%d %H:%M')}")
    
    print(f"\n‚úÖ System would analyze these posts for:")
    print("   ‚Ä¢ Language detection (English/Hindi/Hinglish)")
    print("   ‚Ä¢ Stance towards India (Pro/Anti/Neutral)")
    print("   ‚Ä¢ Toxicity levels")
    print("   ‚Ä¢ Coordination patterns")
    print("   ‚Ä¢ Bot likelihood scores")
    print("   ‚Ä¢ Narrative clustering")
    print("   ‚Ä¢ Temporal burst detection")

def show_system_overview():
    """Show system capabilities overview"""
    print("üõ°Ô∏è  CYBER THREAT DETECTION SYSTEM")
    print("=" * 60)
    print("üéØ CAPABILITIES:")
    print("   ‚úÖ Multi-platform data collection (Twitter, Reddit, YouTube)")
    print("   ‚úÖ Multilingual processing (English, Hindi, Hinglish + 12 more)")
    print("   ‚úÖ Anti-India narrative detection")
    print("   ‚úÖ Toxicity classification")
    print("   ‚úÖ Coordinated behavior detection")
    print("   ‚úÖ Bot network identification")
    print("   ‚úÖ Temporal burst analysis")
    print("   ‚úÖ Campaign threat scoring (0-100)")
    print("   ‚úÖ Real-time alerting")
    print("   ‚úÖ Interactive dashboard")
    print("   ‚úÖ Ethical AI safeguards")
    
    print("\nüèóÔ∏è  ARCHITECTURE:")
    print("   üìä Streamlit Dashboard (Port 8501)")
    print("   üîó FastAPI Backend (Port 8000)")
    print("   üóÑÔ∏è  PostgreSQL Database")
    print("   üï∏Ô∏è  Neo4j Graph Database")
    print("   üî¥ Redis Cache")
    print("   üê≥ Docker Containerized")
    
    print("\nüì¶ DEPLOYMENT:")
    print("   ‚Ä¢ docker-compose up -d")
    print("   ‚Ä¢ python quick_start.py")
    print("   ‚Ä¢ Automated setup and initialization")

def main():
    """Main demo function"""
    show_system_overview()
    
    print("\n" + "="*60)
    print("üöÄ RUNNING COMPONENT DEMOS...")
    print("="*60)
    
    # Run individual component demos
    demo_language_detection()
    demo_stance_detection()
    demo_burst_detection()
    demo_campaign_scoring()
    demo_sample_analysis()
    
    print("\n" + "="*60)
    print("‚úÖ DEMO COMPLETED!")
    print("üìñ For full documentation, see README.md")
    print("‚öôÔ∏è  To start the complete system: python quick_start.py")
    print("üß™ To run tests: python run_tests.py")
    print("="*60)

if __name__ == "__main__":
    main()